---
calendar: ml
post_year: 2019
post_day: 17
title: How to talk like a machine
image: >-
  https://images.unsplash.com/photo-1414389754010-8cf70521937b?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1100&q=60
ingress: >-
  When I got up at dawn, I could feel the sun beating down on my bare body. I
  felt a little sweat rolling down my back, but I didn't really mind. I was
  getting back in shape to train for that night's show. The sun wasn't going to
  be up for another hour, so I was happy to start the day off right.


  When I decided to write a blog post called "How to talk like a machine", I did
  it in as few words as possible. The problem with writing articles, is that
  they are usually a lot longer than your average blog post. I keep editing
  them, because I am a perfectionist. …and that's exactly what I tried to do in
  this post. I thought that maybe writing only a few words might help to
  optimize my words.


  When I decided to write a blog post called "How to talk like a machine", I
  discovered that the question of how to write a simple blog post had a lot of
  deep implications. The follow-up question was "How can I make my blog post
  read like a machine-made blog post?".

  It turns out that there is a difference between writing a single paragraph of
  prose and a long essay in a natural language. There are a few things you need
  to be aware of when you write a machine-made blog post.
links:
  - title: Word2Vec
    url: 'https://en.wikipedia.org/wiki/Word2vec'
  - title: GloVe
    url: 'https://nlp.stanford.edu/projects/glove/'
  - title: BERT
    url: 'https://github.com/google-research/bert'
  - title: Talk to transformer
    url: 'https://talktotransformer.com/'
authors:
  - Jørgen Wilhelmsen
---
Not how you thought this post would begin? The paragraph above was actually generated by a neural network. The neural network is called GPT-2 and is a state of the art text generator. To generate the paragraph above GPT-2 was given the sentence _When I decided to write a blog post called "How to talk like a machine",_ and came up with the rest by it self. The rest of this blogpost will go through some fundamental theory regarding text generators.

A text generator is a computer algorithm that generate text. There are different methods for generating text with computers, but we will focus on approaches based on neural networks in this post. The core goal of text generation can be defined as: 

_Given the text I have seen, what is the text most likely to follow?_

This can easily be translated to a prediction task for a neural network: Feed in some seed text to the network, and have it output the text it thinks is the most likely to follow.

In order to train a network to perform this task, we have to represent words as numbers in some way. Luckily, some smart people have figured out a great way to do this, and that is by using word embeddings. A word embedding is a numeric vector representation of a word. These vector representations are often learned from gigantic amounts of text and captures relationships between words in the language. Some common algorithms for generating word embeddings include Word2Vec, GloVe and BERT. Word embeddings are often used to translate text to vectors for the neural network. 

The network for the text generator is designed in a way that makes it able to take a sequence of words (represented as vectors) as input, and output probabilities for the next word. The network is trained by feeding it sequences of words from a corpus of text we want the network to mimic, and having it predict the word that follows. After the network is trained we can generate text as follows:
1. Feed an initial seed to the network. This can be a sentence or just an empty string.
2. Make the network predict the probabilities for the next word.
3. Choose one of the words with the highest probability and add it to the end of the current sentence.
4. Feed the augmented sentence back to the network and repet the process from step 2.
We can be repeated steps 2-4 as many times as wanted to generate as long a text sequence as we need.

![](/assets/ml_17_pic1.gif)

You now have some intuition for how a text generator works based on a neural network works. If you want to play with the text generator used to generate the text in the start of this post you can visit [https://talktotransformer.com](https://talktotransformer.com) and feed a seed of your own. Have fun!
