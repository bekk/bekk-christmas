---
calendar: ml
post_year: 2019
post_day: 17
title: Make your own text generator
image: >-
  https://images.unsplash.com/photo-1414389754010-8cf70521937b?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1100&q=60
ingress: >-
  When I got up at dawn, I could feel the sun beating down on my bare body. I
  felt a little sweat rolling down my back, but I didn't really mind. I was
  getting back in shape to train for that night's show. The sun wasn't going to
  be up for another hour, so I was happy to start the day off right.
links:
  - title: Word2Vec
    url: 'https://en.wikipedia.org/wiki/Word2vec'
  - title: GloVe
    url: 'https://nlp.stanford.edu/projects/glove/'
  - title: BERT
    url: 'https://github.com/google-research/bert'
  - title: Talk to transformer
    url: 'https://talktotransformer.com/'
authors:
  - JÃ¸rgen Wilhelmsen
---
Not how you thought this post would begin? The paragraph above was actually generated by a neural network. The neural network is called GPT-2 and is a state of the art text generator. This blogpost will go through some fundamental theory regarding text generators, and maybe even inspire you to make your own.

So, what do we mean by a "text generator"? You guessed it. A text generator is a computer algorithm that generate text. There are different methods for generating text with computers, but we will focus on approaches based on neural networks in this post.

The core goal of text generation can be defined as: 

_Given the text I have seen, what is the text most likely to follow?_

This can easily be translated to a prediction task for a neural network: Feed in some seed text to the network, and have it output the text it thinks is the most likely to follow.

In order to train a network to perform this task, we have to represent words as numbers in some way. Luckily, some smart people have figured out a great way to do this, and that is by using word embeddings. A word embedding is a numeric vector representation of a word. These vector representations are often learned from gigantic amounts of text and captures relationships between words in the language. Some common algorithms for generating word embeddings include Word2Vec, GloVe and BERT. Word embeddings are often used to translate text to vectors for the neural network. 

The network for the text generator is designed in a way that makes it able to take a sequence of words (represented as vectors) as input, and output probabilities for the next word. The network is trained by feeding it sequences of words from a corpus of text we want the network to mimic, and having it predict the word that follows. After the network is trained we can generate text as follows:
1. Feed an initial seed to the network. This can be a sentence or just an empty string.
2. Make the network predict the probabilities for the next word.
3. Choose one of the words with the highest probability and add it to the end of the current sentence.
4. Feed the augmented sentence back to the network and repet the process from step 2.
We can be repeated steps 2-4 as many times as wanted to generate as long a text sequence as we need.

![](/assets/ml_17_pic1.gif)

You now have some intuition for how a text generator works based on a neural network works. If you want to play with the text generator used to generate the text in the start of this post you can visit [https://talktotransformer.com](https://talktotransformer.com) and feed a seed of your own. Have fun!
