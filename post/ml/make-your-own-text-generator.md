---
calendar: ml
post_year: 2019
post_day: 17
title: Make your own text generator
image: >-
  https://images.unsplash.com/photo-1414389754010-8cf70521937b?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1100&q=60
ingress: >-
  When I got up at dawn, I could feel the sun beating down on my bare body. I
  felt a little sweat rolling down my back, but I didn't really mind. I was
  getting back in shape to train for that night's show. The sun wasn't going to
  be up for another hour, so I was happy to start the day off right.
links:
  - title: Word2Vec
    url: 'https://en.wikipedia.org/wiki/Word2vec'
  - title: GloVe
    url: 'https://nlp.stanford.edu/projects/glove/'
  - title: BERT
    url: 'https://github.com/google-research/bert'
  - title: Talk to transformer
    url: 'https://talktotransformer.com/'
authors:
  - Jørgen Wilhelmsen
---
Not how you thought this post would begin? The paragraph above was actually generated by a neural network and is a state of the art algorithm for text generation. This blogpost will go through some fundamental theory regarding text generators, and maybe even inspire you to make your own.

Simply put, a text generator is a computer algorithm that generate text. As I’m quite fascinated by neural networks, this blogpost is going to be about how we can use neural networks to generate text. Yay! 

Essentially, the core goal of text generation can be defined as: 

_Given the text I have seen, what is the text most likely to follow?_

This can easily be translated to a prediction task for a neural network: Feed in some seed text to the network, and have it output the text it thinks is the most likely to follow.

In order to train a network to perform this task, we have to represent words as numbers in some way. As we all know, neural networks do numerical computation, so we need some way to translate between words and numbers. Luckily, some smart people have figured out a great way to do this, and that is by using word vectors. Word vectors are just what they sound like - a vector consisting of numbers, one for each word. Some hotshots in the machine learning community have done everyone else a solid, and used neural networks to work out really useful word vectors for pretty much every word there is. Honorable mentions include w2v (Word2Vec), GloVe and BERT. When we want to present some text to a network, we first swap out each word with its corresponding vector (using any of the collections linked to below) before the result is passed to the network. 



The network should be designed in a way that makes it able to take a bunch of numbers (sequence of word vectors) as input, and output a probability distribution that tells us the likelihood of each possible word being the next. To train the network, the probability distribution is compared with the word that actually followed the text presented to the network. In order to generate text, simply add the word with the highest probability to the sentence, and repeat. This can be repeated as many times as wanted to generate text of varying length.

![](/assets/ml_17_pic1.gif)

[https://talktotransformer.com](https://talktotransformer.com)

That was it for my intro to text generation! Thank you for reading.
